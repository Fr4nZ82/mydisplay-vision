# MyDisplay Vision ‚Äì config_explained.md (complete)

Questo documento elenca **tutte le propriet√†** utilizzabili in `config.json`, con spiegazioni pratiche su cosa fanno e quando modificarle.

---

## üé• Capture / Stream

### camera
Indice della webcam da aprire (0 = predefinita).  
Imposta 1, 2, ... se hai pi√π dispositivi. Se assente, il software tenta 0 come default.

### width / height
Risoluzione richiesta alla camera (pixel).  
Pi√π alta = pi√π qualit√† ma pi√π carico (pi√π memoria, pi√π lavoro per detector/classifier).

### target_fps
FPS **desiderati** per cattura+pipeline.  
Abbassalo per ridurre carico CPU, alzalo per tracking pi√π fluido (se la macchina regge).

### debug_enabled
Abilita/disabilita la pagina `/debug` (HTML + stream MJPEG).  
Utile disattivarlo su host headless o quando non serve osservare il video.

### debug_stream_fps
Frequenza dei frame nello stream MJPEG (non influisce sul loop interno).  
Valori bassi riducono banda e CPU lato server/client.

### debug_resize_width
Larghezza del frame nella pagina `/debug` (mantiene aspect ratio).  
0 o negativo = nessun resize. Ridurlo accelera encoding JPEG e render nel browser.

---

## üåê API

### host / port
Indirizzo e porta del server FastAPI.  
`host: "127.0.0.1"` per accesso locale; usa `"0.0.0.0"` per esporre nella LAN.

---

## üîé Rilevatore Volti (YuNet)

### detector_resize_width
Ridimensiona **solo per la detection** (non tocca lo stream di debug).  
Valori tipici 480‚Äì640: riduce molto il carico mantenendo buona qualit√† su volti medi.

### yunet.onnx_path
Percorso del modello ONNX per YuNet face detector.  
Se mancante o invalido, la detection √® disabilitata ‚Üí vedrai solo il video senza riquadri.

### yunet.score_th
Soglia minima di confidenza (0‚Äì1) per accettare un volto.  
Alzala per ridurre falsi positivi; abbassala per cogliere volti lontani/difficili.

### yunet.nms_th
Soglia IoU per la Non‚ÄëMaximum Suppression.  
Bassa = elimina pi√π box sovrapposti; alta = conserva pi√π box vicini.

### yunet.top_k
Numero massimo di box gestiti a valle del detector.  
Aumenta solo in scene molto affollate; impatta la latenza.

---

## üß† Classificatore Et√†/Genere (ONNX)

Hai due modalit√†: **modelli separati** o **modello combinato**.

### (Modelli separati) age_model_path / gender_model_path
Percorsi dei modelli ONNX indipendenti per et√† e genere.  
Se non presenti o non caricabili, si usa il fallback: etichette `unknown` e confidenza `0.0`.

### age_buckets
Elenco delle fasce d‚Äôet√† in output (stringhe mostrate in overlay e metriche).  
Adattale alla tassonomia aziendale (es. 0‚Äë13, 14‚Äë24, ... 65+).

### cls_min_face_px
Lato minimo (pixel) del volto per tentare la classificazione.  
Evita inferenze su volti troppo piccoli (rumorosi) e risparmia CPU.

### cls_min_conf
Confidenza minima per **genere** (sotto soglia ‚Üí `unknown`).  
Utile per essere conservativi in condizioni di luce/angolo difficili.

### cls_interval_ms
Intervallo minimo tra due inferenze sullo **stesso track** (caching).  
Riduce il carico quando una persona resta in scena per pi√π secondi.

---

## üß† Classificatore Combinato (consigliato)

### combined_model_path
Percorso del modello ONNX che predice **et√† e genere insieme** (es. Intel retail‚Äë0013, InsightFace genderage.onnx).  
Se presente, ha priorit√† sui modelli separati (pi√π veloce, una sola inferenza).

### combined_input_size
Dimensione di input del volto per il modello combinato, es. `[62, 62]` (Intel) o `[96, 96]` (InsightFace).  
Deve corrispondere a quanto atteso dal modello.

### combined_bgr_input
Se `true`, il modello attende BGR (tipico OpenCV, Intel/InsightFace); se `false`, attende RGB.  
Consente di evitare conversioni inutili e piccoli errori di colore.

### combined_scale01
Se `true`, i pixel sono scalati a `[0..1]`; se `false`, si lasciano a `[0..255]`.  
Intel retail‚Äë0013 lavora correttamente con 0..255.

### combined_age_scale
Fattore per riportare l‚Äôet√† alla scala reale quando l‚Äôoutput √® **normalizzato** (es. `age_norm * 100`).  
Intel e InsightFace usano spesso et√†/100 ‚Üí lascia 100.0.

### combined_gender_order
Ordine delle classi nell‚Äôoutput di genere del modello combinato, tipicamente `["female","male"]`.  
Serve per interpretare correttamente il vettore probabilit√†/logit a 2 elementi.

---

## üß≠ Tracker (ID stabili)

Il **tracker** mantiene un ID coerente per ogni volto rilevato tra pi√π frame consecutivi.  
Ogni persona rilevata diventa un **track**, cio√® un oggetto con un numero identificativo (`track_id`) che rimane stabile mentre quella persona √® visibile.

Il tracker associa le nuove detection (YuNet) con i track gi√† esistenti confrontando la **IoU (Intersection over Union)** tra riquadri.  
Un‚Äôalta IoU significa che i box si sovrappongono molto ‚Üí probabilmente √® lo stesso volto.  
Un‚ÄôIoU bassa ‚Üí volto diverso o spostamento troppo ampio.

### tracker_max_age
Numero massimo di frame consecutivi senza corrispondenza prima che un track venga eliminato.  
Serve per tollerare ‚Äúbuchi‚Äù momentanei della detection (es. occlusioni, blur, cambi luce).  
- Valore basso (es. 5): pi√π reattivo, ma perde ID se il volto sparisce per un attimo.  
- Valore alto (es. 20‚Äì30): mantiene l‚ÄôID anche se il volto manca per qualche frame.  
Utile quando si vogliono ID persistenti o quando YuNet ha piccoli drop.

### tracker_min_hits
Numero minimo di frame consecutivi necessari prima che un track venga considerato ‚Äúvalido‚Äù.  
Serve a filtrare detections casuali o falsi positivi.  
- Valore basso (1‚Äì2): crea subito ID, ma genera pi√π falsi positivi.  
- Valore alto (3‚Äì5): pi√π stabile, ma introduce ritardo nel riconoscimento iniziale.

### tracker_iou_th
Soglia di **IoU** (0‚Äì1) per considerare una detection come la stessa persona di un track precedente.  
- Pi√π bassa (0.2): pi√π permissiva ‚Üí mantiene ID anche con spostamenti ampi.  
- Pi√π alta (0.6): pi√π rigida ‚Üí pu√≤ perdere ID se il volto si muove molto.  
Valori comuni: **0.3‚Äì0.4**.  
Un buon equilibrio evita di confondere persone vicine, ma mantiene l‚ÄôID coerente.

---

## üö∂ ROI / Tripwire

La **tripwire** √® una linea virtuale tracciata nell‚Äôimmagine per rilevare **attraversamenti** (entrate/uscite).  
Quando il **centro del volto** di un track attraversa questa linea, viene registrato un evento `CROSS` nel sistema di aggregazione.  
√à il principio base del conteggio di passaggi.

### Come funziona
- La linea √® definita da due punti normalizzati `[[x1, y1], [x2, y2]]`, dove 0..1 rappresentano la proporzione rispetto alla larghezza/altezza del frame.  
- Il tracker salva la posizione del centro del volto (prev e curr).  
- Se la linea viene attraversata, viene emesso un evento con direzione `a2b` o `b2a`.  
- Ogni evento include il `track_id`, genere, fascia d‚Äôet√† e direzione.

### roi_tripwire
Coppia di punti normalizzati `[ [x1, y1], [x2, y2] ]` che rappresentano la linea A‚ÜíB.  
Esempi:
- `[ [0.1, 0.5], [0.9, 0.5] ]` ‚Üí linea orizzontale al centro, da sinistra a destra.  
- `[ [0.5, 0.2], [0.5, 0.8] ]` ‚Üí linea verticale al centro, dall‚Äôalto in basso.  
Adatta questi valori per posizionare la linea sopra soglie, porte, ingressi, corridoi, ecc.

### roi_direction
Direzione di conteggio consentita:
- `"both"` ‚Üí conta in entrambe le direzioni (entrate + uscite).  
- `"a2b"` ‚Üí conta solo se il movimento √® da A verso B.  
- `"b2a"` ‚Üí conta solo da B verso A.  
Imposta `"a2b"` o `"b2a"` per contare solo entrate o solo uscite (utile per varchi unidirezionali).

### roi_band_px
Spessore della **banda** attorno alla tripwire (in pixel).  
√à la tolleranza verticale/orizzontale per il rilevamento dell‚Äôattraversamento.  
- Pi√π basso (5): linea sottile ‚Üí preciso ma sensibile al jitter.  
- Pi√π alto (20): fascia larga ‚Üí pi√π robusta ma pu√≤ contare passaggi laterali.  
In pratica, la tripwire non √® una linea infinitamente sottile, ma una fascia che riconosce attraversamenti con margine.

---

## Re-Identification (memoria facce)

### reid_enabled
Abilita o disabilita il riconoscimento ricorrente (Re-ID) dei volti.
Quando true, il sistema calcola un‚Äôimpronta (embedding) del volto e prova a riconoscerlo se riappare dopo essere uscito dall‚Äôinquadratura.
Se il modello face_recognition_sface_2021dec.onnx non √® presente o OpenCV non supporta SFace, la funzione viene ignorata automaticamente.

## reid_model_path
Percorso del modello ONNX per il riconoscimento facciale (embedding) usato dal Re-ID.
√à consigliato usare il modello Intel SFace incluso in OpenCV contrib:
models/face_recognition_sface_2021dec.onnx.

## reid_similarity_th
Soglia di similarit√† (cosine similarity) tra due volti per considerarli la stessa persona.
- Valori pi√π alti ‚Üí meno falsi positivi, ma rischio di non riconoscere la stessa persona con illuminazione diversa.
- Valori pi√π bassi ‚Üí pi√π tolleranza, ma possibili accoppiamenti errati.
Esempio: 0.35 ‚Äì 0.40 √® un buon punto di partenza per SFace.

## reid_cache_size
Numero massimo di volti memorizzabili nella cache del Re-ID.
Ogni voce contiene embedding e timestamp dell‚Äôultima volta che la persona √® stata vista.
Quando il limite √® superato, vengono eliminati i pi√π vecchi.

## reid_memory_ttl_sec
Durata della ‚Äúmemoria‚Äù del Re-ID, in secondi.
Se una persona non viene pi√π vista per pi√π di questo intervallo, la sua impronta viene rimossa e, al successivo ingresso, ricever√† un nuovo ID.
Esempio: 600 = 10 minuti.

## count_dedup_ttl_sec
Intervallo minimo (in secondi) prima che una stessa persona possa essere nuovamente conteggiata dopo un attraversamento della tripwire.
Serve a evitare doppi conteggi per chi rientra subito nell‚Äôinquadratura o passa pi√π volte davanti al display in poco tempo.
Esempio: 600 = 10 minuti di blocco conteggio per la stessa persona.

---

## üìä Metriche / Aggregazione

Ogni attraversamento (evento CROSS) viene inviato al modulo `MinuteAggregator`.  
L‚Äôaggregatore accumula eventi per finestre temporali regolari, permettendo di analizzare flussi di persone nel tempo.

### metrics_window_sec
Durata, in secondi, della finestra temporale per raggruppare eventi.  
Es.: 60 = conteggio per minuto.  
Ogni finestra produce un record con i totali per sesso e fascia d‚Äôet√†.  
Finestra pi√π lunga (es. 300s) ‚Üí dati pi√π stabili ma meno reattivi.

### metrics_retention_min
Tempo di retention dei record di aggregazione (in minuti reali).  
Definisce quanto a lungo le statistiche rimangono in memoria prima di essere scartate.  
Esempio: `metrics_window_sec=60`, `metrics_retention_min=120` ‚Üí memorizza gli ultimi 120 minuti di dati.  
Aumenta per report storici pi√π lunghi o interrogazioni pi√π ampie su `/metrics/minute?last=N`.

---

## üîç Relazione tra tracker e tripwire

1. **YuNet** rileva un volto nel frame.  
2. **Tracker** assegna o aggiorna un `track_id`.  
3. Ogni `track_id` conserva la posizione del volto (centro del box).  
4. Quando il centro attraversa la **tripwire (A‚ÜíB o B‚ÜíA)**, viene generato un evento `CROSS`.  
5. L‚Äôevento alimenta l‚Äô**aggregatore**, che aggiorna i contatori per il minuto in corso.  
6. Le statistiche sono consultabili via `/metrics/minute`.

In sintesi, il tracker mantiene la coerenza nel tempo, la tripwire trasforma il movimento in eventi, e l‚Äôaggregatore li converte in dati analitici aggregati.

---

## üîó Suggerimenti rapidi

- **Performance:** abbassa `detector_resize_width`, `target_fps`, alza `cls_interval_ms`.  
- **Precisione:** alza `yunet.score_th`, riduci `tracker_iou_th`, tieni `cls_min_face_px` ‚â• 64.  
- **Combinato Intel:** `combined_input_size=[62,62]`, `combined_bgr_input=true`, `combined_scale01=false`, `combined_age_scale=100.0`, `combined_gender_order=["female","male"]`.  
- **Combinato InsightFace:** `combined_input_size=[96,96]`, `combined_bgr_input=true` (o `false` se il repo specifica RGB), `combined_scale01=false`, `combined_age_scale=100.0`.
